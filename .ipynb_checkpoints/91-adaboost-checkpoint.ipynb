{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remember to show the cost function of adaboost vs gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hypothesis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll let $f_m$  represent singly weak learner or decision tree, and we'll let $F(x)$ represent the adaboost model.  We'll have a positive target be represented by $1$, and a negative be represented by $-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a classification problem, adaboost then makes predictions by each decision tree voting on the prediction of each observation.  The votes are weighted the cost of each decision tree, with those with a lower cost getting a weight.  This is what the hypothesis function looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F(x) = sign(\\alpha_1*f_1(x) + \\alpha_2*f_2(x) ... \\alpha_M*f_m(x)) = sign \\left(  \\sum_{m=1}^M \\alpha_m*f_m(x) \\right ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after adding up the weighted vote for each observation, if the sum is positive the adaboost model predicts 1, and if the sum is negative the model predicts -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move onto how an adaboost model trains.  With adaboost, trees are trained one after the other.  After the first tree is trained, the next tree places a higher weight on training to the observations that the previous tree predicted incorrectly.  So each tree trains to the weaknesses of the previsouly trained tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see this if we look at the graph below, illustrating this process of placing higher weights on the misclassified observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./successive-trees.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From [Adaboost Intuition](https://xavierbourretsicotte.github.io/AdaBoost.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It's a voting algorithm \n",
    "\n",
    "2. Not everyone gets equal votes\n",
    "\n",
    "3. Then we weight the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, going from top left to bottom right, in the first square each observation receives equal weight.  We can see that the classifier properly classified each of the red observations but missed on some of the lower blue observations.  In the next second square, these improperly blue observations were weighted higher, and this time properly classified.  \n",
    "\n",
    "This procedure continues.  Each time, the decision tree trains by assigning higher weight to the observations previously trained incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onto the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the pseudocode for the adaboost algorithm.  Let's look over it now.  Then we'll develop a deeper understanding of it as we implement it step by step in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./adaboost.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, bool_y = make_moons(n_samples=3000, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.where(bool_y == 0, -1, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "w = np.ones(y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train, sample_weight = w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = dtr.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Calculate error of t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08088888888888889"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_t = w[y_hat != y_train].sum()/y_train.shape[0]\n",
    "error_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_t = .15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The higher the error, the lower the value of alpha.  In other words, alpha is how well the learner performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2151652742757622"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = .5*np.log((1 - error_t)/error_t)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then find the new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29040893, 0.29040893, 0.29040893, ..., 0.29040893, 0.29040893,\n",
       "       0.29040893])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w*np.exp(-alpha*y_train*y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Is it correct, `y_train*y_hat` (positive when correct, negative when incorrect)\n",
    "> When correct, multiply weight by fraction, when incorrect to exponent.  \n",
    "\n",
    "> If the model performs well, but incorrectly classified this, then weight even higher.  If model performs well, and correctly classified, then weight even lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9222222222222223"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_train==y_hat).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Many Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's loop through this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ws = []\n",
    "alphas = []\n",
    "y_hats = []\n",
    "errors = []\n",
    "dtcs = []\n",
    "w = np.ones(y_train.shape[0])/y_train.shape[0]\n",
    "\n",
    "for i in range(30):\n",
    "    dtc = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train,\n",
    "                                                    sample_weight = w)\n",
    "    y_hat = dtc.predict(X_train)\n",
    "    error_t = w[y_hat != y_train].sum()/y_train.shape[0]\n",
    "    alpha = .5*np.log((1 - error_t)/error_t)\n",
    "    w = w*np.exp(-alpha*y_train*y_hat)\n",
    "    w = w/w.sum()\n",
    "    ws.append(w)\n",
    "    alphas.append(alpha)\n",
    "    y_hats.append(y_hat)\n",
    "    errors.append(error_t)\n",
    "    dtcs.append(dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_arr = np.array(alphas)\n",
    "tree_preds = np.array(y_hats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2250)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dtrs, alphas, X):\n",
    "    preds = np.vstack([alpha*dtr.predict(X) for dtr, alpha in zip(dtrs, alphas)])\n",
    "    cum_preds = preds.sum(axis = 0)\n",
    "    return np.where(cum_preds > 0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(dtcs, alphas, X_test)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9972222222222222, 0.9944598337950139)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_test, predictions), recall_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Building Adaboost from scratch](https://geoffruddock.com/adaboost-from-scratch-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Adaboost Summary](https://xavierbourretsicotte.github.io/AdaBoost.html)\n",
    "\n",
    "[Boosting MIT Lecture](https://www.youtube.com/watch?v=UHBmv7qCey4)\n",
    "\n",
    "[Adaboost TA Session](https://www.youtube.com/watch?v=gmok1h8wG-Q&t=3s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

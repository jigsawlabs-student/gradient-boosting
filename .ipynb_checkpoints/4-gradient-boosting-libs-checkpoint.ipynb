{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With xgboost, the goal is a little different because it \n",
    "\n",
    "* XGboost provides a heuristic for a loss function, and regularization, and model complexity controlled by gamma, which controls number of leafs\n",
    "\n",
    "* And to reduce model complexity, gamma is introduced, \n",
    "\n",
    "And then we have l2 regularization of the instances with teh weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Splitting is regularized \n",
    "    * So they calculate the gain of a split, and if the gain is good enough then the split is done, and if not greater than gamma, then the split is not done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* And this is how we control tree growth while building the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost - written in 2014, and in 2016, became famous on kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences with Lightgbm and Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lightgbm - written by microsoft. \n",
    "    * Written in C++, and can be 3-4X faster than XGboost.\n",
    "    * Use instance weights.  Provides new insight for instance weight.\n",
    "    * Gradient based one side sampling. And here define important instances in terms of their gradient.  \n",
    "    * So if a gradient for an instance for large - because it influences the loss function a lot.  \n",
    "    * So will look at 10k most important, and then for remaining they just sample all other instances.\n",
    "     \n",
    "1. So don't estimate gradient with respect to whole dataset\n",
    "2. Subsample instances with high gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lightgbm is used same as xgboost, it's just faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use if have lot of categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Developed by yandex, a russian google\n",
    "* Designed to work with categorical variables, and they calculate mean targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean target encoding \n",
    "    > Calculate average values of a target for some categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, light gbm is faster than xgboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kaggle Forest Cover Type Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If number of features is not greater than about 1000, good to try random forest or xgboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try logisitic regression and random forest to see how they perform.  Not very sensitive to hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multiclass logistic regression, so pass it multinomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of leaves is $2^{depth} -  1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If lower the learning rate, and increase number. of iterations, we have better convergance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, really can set iterations to 1000 or 3000.  But here just set it to 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then received a slightly higher accuracy, 86.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do two stage hyperparameter tuning \n",
    "1. Max depth, num leaves, regularization\n",
    "2. Responsible for convergence - this is learning rate and num iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

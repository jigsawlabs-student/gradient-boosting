{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Gradient Boosting Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how the gradient boosting technique works, it's time to spend time exploring the different implementations of gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost and LightGbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is the XGBoost library. XGboost was written in 2014, became famous through it's success on Kaggle in 2016.  One of the top hyperparemeters in XGboost is regularization through gamma.  The basic idea is to control for model complexity, and thus for variance, by only performing a split if there is enough gain from doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightgbm is a essentially a rewrite of the XGboost library.  The main benefit of LightGBM is that it's 3-4x faster than XGboost.  It accomplishes this by avoiding calculating the gradient based on the entire dataset, but instead just looking at the calculating the gradients based on the most 10,000 influential observations.  It the samples from the remaining observations and incorporates the sampled observations into it's calculation of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost was developed by Yandex, which is the Google of Russia.  It's called catboost because it's main feature is the way that it handles categorical variables.  It does so with a technique called mean target encoding.  \n",
    "\n",
    "Remember that when we have decision trees, we will have better splits if we are able to order our categorical features in some way.  If we can order them, then we can have a split that says that, say, movies that are ranked 4 stars and above will have a higher score.  But without categorical ordering, there is no ranking of our categories that provides for a potentially easy split.  \n",
    "\n",
    "So through mean target encoding instead of encoding a movie genre as Action, we encode it as the average amount that an action movie makes.  This again allows us to provide an order to our features, from least lucrative to more lucrative genres.  This is the general idea, but there are further details that we'll explore in future lessons.\n",
    "\n",
    "There are other techniques that catboost provides out of the box.  For example, it only one hot encodes categorical data if there are fewer than five values per feature.  It combines different categorical features out of the box, so that different interactions can be captured.  And it allows for training and evaluating to optimize a variety of cost function functions.  For this reason, catboost will be our tool of choice in gradient boosting libraries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

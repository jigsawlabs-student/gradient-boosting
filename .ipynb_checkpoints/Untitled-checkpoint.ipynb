{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In random forest, we build each of the trees independently.  \n",
    "    * But here, we build the composition iteratively.\n",
    "2. Adaboost\n",
    "    * In adaboost, the samples have different weights. \n",
    "    * And the idea is that we weigh the instances that were misclassified.\n",
    "    \n",
    "* And each tree, we weigh each tree.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can quickly build non-trivial non-linear surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pseudo-ada.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Weighing each tree\n",
    "* if epsilon_t is low, then alpha will be close to 1.\n",
    "* Or if it's bigger than it's lower.\n",
    "\n",
    "The lower training error, the higher is alpha t.  The weight in the iteration of the learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Weighing   the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Margins - \n",
    "    * b_t is base algorithm, and multiply it by y_i, which is called margin.\n",
    "    \n",
    "    $w^{(t)} = w^{t-1}e^{-\\alpha_t  * y_i* b_t(x_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When margin < 0, it's a misclassification.\n",
    "* Absolute value of margin, is certainty in classification.  So if absolute value is high, then we were certain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If misclassification, then margin is negative, so then we have :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $w*e^+$, so will be increasing the weight\n",
    "* $w*e^-$, so will be decreasing weight\n",
    "\n",
    "And this is proportional to the strength $\\alpha_t$, the strength of the learner at iteration t.  So strong learners influence weights more.  And if the learner sucks, then we won't influence weights very much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\alpha_t $ strength of the learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we then normalize the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drawback with adaboost is that if there are some labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the loss function, can see if have outliers, adaboost has exponentially large weights for the outliers. So the outliers will enlarge, but will again misclassify them.  \n",
    "\n",
    "So it's very suscepitable to outliers.  \n",
    "\n",
    "So then they started to look at modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So this boosting really is just variants of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So then gradient boosting is really just a generalization of these approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* And it minimizes any loss function which is differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friedman's GBM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll move through the boosting algorithm.  As we'll see, it operates by training a series of successive of estimators, with each estimator fitting to the error of the current gradient boosting model.  In this lesson, we'll break down how this works by seeing gradient boosting in action.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll explore XGboost by working with data from the California Housing dataset.  Let's begin by loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "ca_housing = fetch_california_housing()\n",
    "\n",
    "X = pd.DataFrame(ca_housing['data'], columns = ca_housing['feature_names'])\n",
    "y = pd.Series(ca_housing['target'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move onto building a gradient boosting machine.  Like a random forest, a gradient boosting machine is an ensemble function built from a series of estimators.  The first estimators is just a vector of the sample means.  Each successive estimator is a decision tree trained on the residuals of the current gradient boosting model, multiplied by a learning rate.  Let's see move through this step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A vector of the sample means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step in training the model, our first estimator is simply to estimate the sample mean, for every observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.06582754, 2.06582754, 2.06582754, ..., 2.06582754, 2.06582754,\n",
       "       2.06582754])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_1 = np.full(y_train.shape[0], y_train.mean())\n",
    "m_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then find the residual and train a decision tree on the residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll calculate the difference between what our gradient boosting machine current predicts, and the observed target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.95082754, -1.48082754, -0.35182754])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_r = y_train.values - m_1\n",
    "\n",
    "ps_r[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The psr stands for pseudo-residuals.  We call these differences pseudo-residuals, as they are not residuals of the final model, but only the current state of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ok, next is the crucial step.  We train a decision tree to fit to these residuals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_1 = DecisionTreeRegressor(min_samples_leaf = 5).fit(X_train, ps_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_1_predictions = dtr_1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.92282754, -1.42107754, -0.43060532])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr_1_predictions[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sum the estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now adding the our initial estimator of means, plus the second estimator that predicts to the residuals, we can get close to predicting our true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9067864218499235"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "F_preds = m_1 + dtr_1_predictions\n",
    "r2_score(y_train, F_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to correct for variance in our decision trees, we don't want to make predictions with just a *couple* of decision trees, but with many.  So we don't want to close the gap between what we predict and what we observe in just a couple of steps.  To avoid doing this, we multiply each decision tree's predictions by a learning rate, like .01.\n",
    "\n",
    "Again let's see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we want to prevent just a overfitting to the training data, by only using a couple of decision trees in our gradient boosting machine.  And the current problem is that in just a couple of trees we can very close to predicting our observed data, leaving little work for later trees to perform.  To correct for this, we only update our predictions by a small learning rate with each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_preds = m_1 + .01*dtr_1_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018045049794813472"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, F_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that, while we have improved upon just predicting the sample mean, we still have a ways to go.  Let's add one more decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9004229 , -1.3927981 , -0.42203627, -0.57136928])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psr_2 = y_train - F_preds\n",
    "dtr_2 = DecisionTreeRegressor(min_samples_leaf = 5).fit(X_train, psr_2.values)\n",
    "dtr_2_predictions = dtr_2.predict(X_train)\n",
    "dtr_2_predictions[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_preds = m_1 + .01*dtr_1_predictions + .01*dtr_2_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03573329903782729"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, F_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that we are slowly improving our score with each decision tree we add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's write our code in such a way that we can train many trees.  First let's start with our hypothesis function.  We left off with our hypothesis function looking like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_preds = m_1 + .01*dtr_1_predictions + .01*dtr_2_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's rewrite it so that we iterate through our decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [dtr_1, dtr_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_train, y_train, estimators = [], lr = .01):\n",
    "    prediction = np.full(y_train.shape[0], y_train.mean())\n",
    "    for estimator in estimators:\n",
    "        prediction += lr*estimator.predict(X_train) \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.04759503, 2.03768878, 2.05730112, ..., 2.05954744, 2.0804158 ,\n",
       "       2.11751168])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict(X_train, y_train, estimators)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03555915220426964"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we wound up with the same score as previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's write a function called train.  \n",
    "\n",
    "> Remember that it should repeatedly train on the pseudo-residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, n_estimators = 1):\n",
    "    dtrs = []\n",
    "    for i in range(n_estimators):\n",
    "        predictions = predict(X, y, dtrs)\n",
    "        psr = y - predictions\n",
    "        dtr = DecisionTreeRegressor(min_samples_leaf = 5).fit(X,psr)\n",
    "        dtrs.append(dtr)\n",
    "    return dtrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code aboove, we first call predict, which on the first iteration just predicts the sample mean.  Then we find the pseudoresiduals, and train our decision tree to those pseudoresiduals.  Finally, we add the decision tree to the list of `dtrs`.  When we make the predictions in the next iteration, our predict function multiplies the predictions by a learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's see if our function works by training two trees, and seeing if we get the same score as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = train(X_train, y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(X_train, y_train, trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03573330886692294"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have our train and predict functions, let's build train an model with 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_200 = train(X_train, y_train, n_estimators=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the trees, let's evaluate our model by predicting and then scoring on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(X_test, y_test, trees_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.17851824, 2.47939638, 1.78179072, ..., 2.9055769 , 1.46779453,\n",
       "       1.33473162])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7788355274850235"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well this performs against a random forest regressor with min_sample_leaf of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(min_samples_leaf=5, n_estimators = 200).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8082808501432994"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that even our simple version of gradient boosting compares well with our randomforest regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned about the gradient boosting alogorithm.  We saw that with gradient boosting we train a series of estimators on the pseudoresiduals of our model.  We went through the steps of gradient boosting where our first estimator is a vector of mean values, and then we train each successive estimator on the pseudoresiduals of the current model.  We multiply the predictions of each estimator by learning rate so that our model can slowly improve and we can train more estimators as we close the gap between what our model predicts and what is observed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
